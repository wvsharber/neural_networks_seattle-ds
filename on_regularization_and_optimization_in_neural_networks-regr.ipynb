{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization and Optimization in Neural Networks\n",
    "\n",
    "## Regularization of NNs\n",
    "\n",
    "Does regularization make sense in the context of neural networks? <br/>\n",
    "\n",
    "Yes! We still have all of the salient ingredients: a loss function, overfitting vs. underfitting, and coefficients (weights) that could get too large.\n",
    "\n",
    "But there are now a few different flavors besides L1 and L2 regularization. (Note that L1 regularization is not common in the context of  neural networks.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = pd.read_csv('wine.csv')\n",
    "wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = wine.drop('quality', axis=1)\n",
    "y = wine.quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine['quality'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    random_state=11, stratify=y)\n",
    "ss = StandardScaler()\n",
    "X_train_s = ss.fit_transform(X_train).astype(np.int32)\n",
    "X_test_s = ss.transform(X_test).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "n_input = X_train_s.shape[1]\n",
    "\n",
    "model.add(Dense(n_input, activation='relu'))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the way, here is a helpful blog post that goes carefully through a list of similarly-named different activation functions and loss functions: https://gombru.github.io/2018/05/23/cross_entropy_loss/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train_s, np.array(y_train),\n",
    "                    validation_data=(X_test_s, np.array(y_test)),\n",
    "                   epochs=30, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='Train loss')\n",
    "plt.plot(history.history['val_loss'], label='Test loss')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(abs(np.array(model.predict(X_test_s).T) - np.array(y_test))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a helpful review article on regularization techniques: https://towardsdatascience.com/regularization-in-machine-learning-connecting-the-dots-c6e030bfaddd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_r = Sequential()\n",
    "\n",
    "n_input = X_train_s.shape[1]\n",
    "\n",
    "model_r.add(Dense(n_input, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(0.01)))\n",
    "model_r.add(Dense(1,\n",
    "                 kernel_regularizer=regularizers.l2(0.01)))\n",
    "\n",
    "model_r.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "history_r = model_r.fit(X_train_s, np.array(y_train),\n",
    "                        validation_data=(X_test_s, np.array(y_test)),\n",
    "                       epochs=42, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_r.history['loss'], label='Training loss')\n",
    "plt.plot(history_r.history['val_loss'], label='Testing loss')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examining Our Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_r.predict(X_test_s[:10]).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_r.predict(X_test_s).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.array(model_r.predict(X_test_s).T) - np.array(y_test))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(abs(np.array(model_r.predict(X_test_s).T) - np.array(y_test))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a new regularization idea: Turn some neurons off during training. We'll assign probabilities of 'dropout' and then let fate decide.\n",
    "\n",
    "$\\rightarrow$ Why is this a good idea? *Is* it a good idea?\n",
    "\n",
    "Was this sort of regularization available to us before? Why (not)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_d = Sequential()\n",
    "\n",
    "n_input = X_train_s.shape[1]\n",
    "\n",
    "model_d.add(Dense(n_input, activation='relu'))\n",
    "model_d.add(Dropout(0.2))\n",
    "model_d.add(Dense(1))\n",
    "\n",
    "model_d.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "history_d = model_d.fit(X_train_s, np.array(y_train),\n",
    "                        validation_data=(X_test_s, np.array(y_test)),\n",
    "                        epochs=42, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_d.history['loss'], label='Training loss')\n",
    "plt.plot(history_d.history['val_loss'], label='Testing loss')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history_d.history['acc'][-1], history_d.history['val_acc'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(abs(np.array(model_d.predict(X_test_s).T) - np.array(y_test))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another idea is to try to terminate the training process early, even before some pre-specified number of epochs.\n",
    "\n",
    "$\\rightarrow$ Why is this a good idea? *Is* it a good idea?\n",
    "\n",
    "Was this sort of regularization available to us before? Why (not)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_es = Sequential()\n",
    "\n",
    "n_input = X_train_s.shape[1]\n",
    "n_hidden = n_input\n",
    "\n",
    "model_es.add(Dense(n_hidden, input_dim=n_input, activation='relu'))\n",
    "model_es.add(Dense(1))\n",
    "\n",
    "model_es.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=1e-08, patience=0, verbose=1,\n",
    "                           mode='auto')\n",
    "\n",
    "callbacks_list = [early_stop]\n",
    "\n",
    "history_es = model_es.fit(X_train_s, np.array(y_train),\n",
    "                          validation_data=(X_test_s, np.array(y_test)),\n",
    "                         epochs=40, batch_size=None, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_es.history['loss'], label='Training loss')\n",
    "plt.plot(history_es.history['val_loss'], label='Testing loss')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(abs(np.array(model_es.predict(X_test_s).T) - np.array(y_test))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build your own network *with some sort of regularization built in* to predict digits using sklearn's `load_digits` dataset!\n",
    "\n",
    "The imports you need are in the next cell.\n",
    "\n",
    "Here are a couple hints and leading questions:\n",
    "\n",
    "1. You'll need to use `to_categorical()` on your target. (What does this function do?)\n",
    "2. What should your output layer look like? How many neurons should it have and what should your activation function be there?\n",
    "3. When we compile this network, what loss function should we use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_digits()\n",
    "print(data.data)\n",
    "print(data.target)\n",
    "print(data.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(data.images[0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
